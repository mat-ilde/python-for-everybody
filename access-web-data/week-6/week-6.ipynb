{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data=\"\"\"{\n",
    "    \"name\":\"chuck\",\n",
    "    \"phone\":{\n",
    "        \"type\":\"intl\",\n",
    "        \"number\":\"+17 343 034 456\"\n",
    "        },\n",
    "        \"email\":{\n",
    "            \"hide\":\"yes\"\n",
    "        }\n",
    "}\"\"\"\n",
    "\n",
    "info=json.loads(data)\n",
    "print(\"Name:\",info[\"name\"])\n",
    "print(\"Hide:\",info[\"email\"][\"hide\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input=\"\"\"[\n",
    "    {\"id\":\"001\",\n",
    "        \"x\":\"2\",\n",
    "        \"name\":\"chuck\"\n",
    "    },\n",
    "    {\"id\":\"009\",\n",
    "        \"x\":\"7\",\n",
    "        \"name\":\"chuck\"\n",
    "    }\n",
    "]\"\"\"\n",
    "info=json.loads(input)\n",
    "print(\"User count:\",len(info))\n",
    "for item in info:\n",
    "    print(\"Name:\",item[\"name\"])\n",
    "    print(\"Id:\",item[\"id\"])\n",
    "    print(\"Attribute:\",item[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final_work_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "file = open(\"trabajo_1access.json\")\n",
    "data = json.load(file)\n",
    "num=(data[\"comments\"],[\"name\"])\n",
    "name=num[0]\n",
    "suma=0\n",
    "for i in name:\n",
    "    final=(i[\"count\"])\n",
    "    final=int(final)\n",
    "    suma=suma+final\n",
    "print(suma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other way to do it  using urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "url=\"http://py4e-data.dr-chuck.net/comments_353805.json\"\n",
    "file=urllib.request.urlopen(url)\n",
    "data = json.load(file)\n",
    "name=(data[\"comments\"],[\"name\"])\n",
    "name=list(name)\n",
    "new=name[0]\n",
    "suma=0\n",
    "for i in new:\n",
    "    num=i[\"count\"]\n",
    "    num=int(num)\n",
    "    suma=suma+num\n",
    "print(suma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final_work_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pràctica con la API de twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "/home/mati/Documentos/python-for-everybody/access-web-data/week-6/api.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"El Gobierno advierte de que la recuperación no llegará a algunos sectores clave hasta final de año\" . El País\n"
     ]
    }
   ],
   "source": [
    "def get_headlines(url):\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    html=urllib.request.urlopen(url).read()\n",
    "    soup=BeautifulSoup(html, \"html.parser\")\n",
    "    s=soup.find_all(class_=\"headline\")\n",
    "    for i in s:\n",
    "        i=str(i)\n",
    "        j=i.split(\">\")\n",
    "        m=j[2]\n",
    "        final=m.split(\"</a\")\n",
    "        text=final[0]\n",
    "        post='\"'+text+'\"'+\" . \"+\"El País\"\n",
    "        return post\n",
    "url=\"https://elpais.es\"\n",
    "titular=get_headlines(url)\n",
    "print(titular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "api = tweepy.API(auth)\n",
    "result=api.update_status(titular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming the hour fot post in twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Hora en Madrid es: 17:47:57\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "hora=time.strftime(\"%X\")\n",
    "hour=\"La Hora en Madrid es: \" + hora\n",
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "api = tweepy.API(auth)\n",
    "result=api.update_status(hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logging, the logs that send me the crontab, i can do that look better\n",
    "# i can control more o less the messages that i can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:This will get logged\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.debug('This will get logged')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:This is a debug message\n",
      "INFO:root:This is an info message\n",
      "WARNING:root:This is a warning message\n",
      "ERROR:root:This is an error message\n",
      "CRITICAL:root:This is a critical message\n"
     ]
    }
   ],
   "source": [
    "logging.debug('This is a debug message')\n",
    "logging.info('This is an info message')\n",
    "logging.warning('This is a warning message')\n",
    "logging.error('This is an error message')\n",
    "logging.critical('This is a critical message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Abriendo página del país\n"
     ]
    }
   ],
   "source": [
    "logging.info('Abriendo página del país')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:This will get logged\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.debug('This will get logged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Mati\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info('Mati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Titular duplicado\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info('Iniciando el proceso...')\n",
    "def get_headlines(url):\n",
    "    logging.info('Leyendo URL' + url)\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    html=urllib.request.urlopen(url).read()\n",
    "    soup=BeautifulSoup(html, \"html.parser\")\n",
    "    s=soup.find_all(class_=\"headline\")\n",
    "    for i in s:\n",
    "        i=str(i)\n",
    "        j=i.split(\">\")\n",
    "        m=j[2]\n",
    "        final=m.split(\"</a\")\n",
    "        text=final[0]\n",
    "        post='\"'+text+'\"'+\" . \"+\"El País\"\n",
    "        return post\n",
    "url=\"https://elpais.es\"\n",
    "titular=get_headlines(url)\n",
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "api = tweepy.API(auth)\n",
    "try:\n",
    "    result=api.update_status(titular)\n",
    "except Exception as e:\n",
    "    logging.error(\"Titular duplicado\", exc_info=False)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<form action=\"https://mobile.twitter.com/i/nojs_router?path=%2Fpascualmati%2Fstatus%2F1248282970026381312\" class=\"NoScriptForm\" method=\"POST\">\n",
    "<input name=\"authenticity_token\" type=\"hidden\" value=\"311aca4d46f163d7c4050b41013b97b118e76251\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
