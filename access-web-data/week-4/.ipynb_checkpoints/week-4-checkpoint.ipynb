{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing simple strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ord(\"H\"))\n",
    "print(ord(\"e\"))\n",
    "print(ord(\"l\"))\n",
    "print(ord(\"o\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python strings to bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response =requests.get('http://data.pr4e.org/intro-short.txt')\n",
    "text=response.text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert from string to bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Maria lava la ropa\"  \t\t\t#string\n",
    "data = \"Maria lava la ropa\".encode()\t#bytes\n",
    "data = b\"Maria lava la ropa\" \t\t\t#bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert from bytes to strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = b\"Maria lava la ropa\"  \t\t#bytes\n",
    "data = b\"Maria lava la ropa\".decode() #string\n",
    "data = str(b\"Maria lava la ropa\")  \t#string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Urllib in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es una libreria para leer paginas de internet en forma de str\n",
    "#importamos la libreria\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "# creamos una variable para abrirla poniendo la direcciÃ²n a la que queremos acceder\n",
    "fhand= urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "# creamos un bucle la pasamos a bytes y nos mostrarÃ  el contenido\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand= urllib.request.urlopen(\"http://www.dr-chuck.com/page1.htm\")\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "url=input(\"Enter -\")\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, \"html.parser\")\n",
    "tags=soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using \"href\" is converted in a  link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href=\"http://www.dr-chuck.com/page1.htm\"\n",
    "print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other way of scrape web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    " #http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final_work_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "suma=0\n",
    "numeros=list()\n",
    "fhand= urllib.request.urlopen(\"http://py4e-data.dr-chuck.net/comments_353802.html\")\n",
    "soup=BeautifulSoup(fhand, \"html.parser\")\n",
    "soup.find_all(class_=\"comments\")\n",
    "s=soup.find_all(\"span\")\n",
    "for num in s:\n",
    "    s=str(s)\n",
    "    y=re.findall(\"[0-9]+\",s)\n",
    "for i in y:\n",
    "    i=int(i)\n",
    "    suma=suma+i\n",
    "print(suma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final_work_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "url=input(\"Enter -\")\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, \"html.parser\")\n",
    "tags=soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\"))\n",
    "# the string parts has to have \"\"\n",
    "# if not back us an error\n",
    "Enter URL: http:http://py4e-data.dr-chuck.net/known_by_Leetisha.html\n",
    "Enter count:6\n",
    "Enter position:7 \n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Maude.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Skie.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Calin.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Briony.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Zack.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Rohin.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Mirryn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this case i looked names and next step was sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "url=\"http://py4e-data.dr-chuck.net/comments_353802.html\"\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, \"html.parser\")\n",
    "name=soup.find_all(\"td\")\n",
    "final=list()\n",
    "for i in name:\n",
    "    i=str(i)\n",
    "    y=re.findall(\"[A-a-Z-z]+\",i)\n",
    "    x=y[1]\n",
    "    if x!=\"span\" and x!=\"Comments\" and x!=\"Name\":\n",
    "        final.append(x)\n",
    "        final.sort()\n",
    "for people in final:\n",
    "    print(people)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the before algorithm now i look for numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "url=\"http://py4e-data.dr-chuck.net/comments_353802.html\"\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, \"html.parser\")\n",
    "s=soup.find_all(\"span\")\n",
    "suma=0\n",
    "for i in s:\n",
    "    s=str(s)\n",
    "    x=re.findall(\"[0-9]+\",s)\n",
    "for n in x:\n",
    "    print(n)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This way i connect my twitter with  the API for post the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key=[\"BtHNJMURBTjLgq5WTBJqKRZ92\"],\n",
    "                  consumer_secret=[\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\"],\n",
    "                  access_token_key=[\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\"],\n",
    "                  access_token_secret=[\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This way i do a get or call for a newspaper page.\n",
    "# the next step is scraping de page\n",
    "# and this form i have the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "url=\"https://elpais.com\"\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, \"html.parser\")\n",
    "s=soup.find_all(class_=\"headline\")\n",
    "for i in s:\n",
    "    i=str(i)\n",
    "    j=i.split(\">\")\n",
    "    m=j[2]\n",
    "    final=m.split(\"</a\")\n",
    "    text=final[0]\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
